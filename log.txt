[2026-02-08 20:37:42] INFO inference_core.py:103: Ê®°ÂûãÁõÆÂΩï: Ê†πÁõÆÂΩï=/pub/data/jiafq/rzm/project/wlw_llm_inference/models, Ollama=/pub/data/jiafq/rzm/project/wlw_llm_inference/models/ollama, HF=/pub/data/jiafq/rzm/project/wlw_llm_inference/models/HF
[2026-02-08 20:37:42] INFO inference_core.py:113: Â∑≤‰ªé config ËÆæÁΩÆ HF_TOKENÔºàÁî®‰∫é gated/ÁßÅÊúâÊ®°ÂûãÔºâ
[2026-02-08 20:37:42] INFO llm_inference.py:986: ÂêØÂä® API ÊúçÂä°: 0.0.0.0:8000
INFO:     Started server process [2840659]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=e2199b90-ea45-40f4-9c44-570f1e497be0 path=/ status=200
INFO:     127.0.0.1:42932 - "GET / HTTP/1.1" 200 OK
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=db247f18-d8be-403e-80aa-28c13e29eeb0 path=/css/style.css status=304
INFO:     127.0.0.1:42932 - "GET /css/style.css HTTP/1.1" 304 Not Modified
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=c839d325-6688-4bb4-8516-c1b34b857e27 path=/js/app.js status=200
INFO:     127.0.0.1:42940 - "GET /js/app.js HTTP/1.1" 200 OK
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=5d31e4f6-4475-41b2-adf6-a6fd0c25af66 path=/api/v1/models status=200
INFO:     127.0.0.1:42940 - "GET /api/v1/models HTTP/1.1" 200 OK
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=778cf98c-1975-4990-9f64-adfce9e6eb38 path=/.well-known/appspecific/com.chrome.devtools.json status=404
INFO:     127.0.0.1:42950 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
[2026-02-08 20:37:53] INFO llm_inference.py:459: request_id=9305adef-33f4-4d17-976d-5dd0fe14c039 path=/api/v1/models/running status=200
INFO:     127.0.0.1:42932 - "GET /api/v1/models/running HTTP/1.1" 200 OK
[2026-02-08 20:38:10] INFO llm_inference.py:162: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced base_url=http://localhost:11434 model=qwen2:0.5b ÂºÄÂßãÊ£ÄÊü•ÊúçÂä°
[2026-02-08 20:38:10] INFO llm_inference.py:185: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced Â∑≤ÁôªËÆ∞ RUNNING_INSTANCESÔºåÂΩìÂâçÊÄªÊï∞=1
[2026-02-08 20:38:10] INFO llm_inference.py:187: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced Ê≠£Âú®Ê£ÄÊü•/ÊãâÂèñÊ®°Âûã‚Ä¶
[2026-02-08 20:38:10] INFO llm_inference.py:84: [Ollama] /api/tags ÊàêÂäü base_url=http://localhost:11434 Â∑≤ÊúâÊ®°Âûã: ['qwen2:0.5b', 'llama3.2:latest']
[2026-02-08 20:38:10] INFO llm_inference.py:117: [Ollama] Ê®°ÂûãÂ∑≤Â∞±Áª™ base_url=http://localhost:11434 model=qwen2:0.5b
[2026-02-08 20:38:10] INFO llm_inference.py:190: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced Ê≠£Âú®ÂàõÂª∫ LLMInferencer‚Ä¶
[2026-02-08 20:38:10] INFO inference_core.py:287: Ollama Â∑≤ÊãâÂèñÊ®°ÂûãÂàóË°® base_url=http://localhost:11434 Êï∞Èáè=2 ÂàóË°®=['qwen2:0.5b', 'llama3.2:latest']
[2026-02-08 20:38:10] INFO inference_core.py:498: LLMInferencer ÂàùÂßãÂåñÊàêÂäü: engine=ollama, model=qwen2:0.5b
[2026-02-08 20:38:10] INFO llm_inference.py:196: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced LLMInferencer ÂàõÂª∫ÊàêÂäüÔºåÊ≠£Âú®È™åËØÅÂèØÁî®ÊÄß
[2026-02-08 20:38:10] INFO inference_core.py:287: Ollama Â∑≤ÊãâÂèñÊ®°ÂûãÂàóË°® base_url=http://localhost:11434 Êï∞Èáè=2 ÂàóË°®=['qwen2:0.5b', 'llama3.2:latest']
[2026-02-08 20:38:10] INFO inference_core.py:287: Ollama Â∑≤ÊãâÂèñÊ®°ÂûãÂàóË°® base_url=http://localhost:11434 Êï∞Èáè=2 ÂàóË°®=['qwen2:0.5b', 'llama3.2:latest']
[2026-02-08 20:38:12] INFO llm_inference.py:202: [Ollama ÂêØÂä®] run_id=97d60f69-cd63-4af1-a347-7414eb03fced Â∑≤ËÆæÁΩÆ inferencerÔºåRUNNING_INSTANCES ‰øùÁïô
[2026-02-08 20:38:12] INFO llm_inference.py:211: Â∑≤ÂêØÂä® Ollama Ê®°Âûã uid=97d60f69-cd63-4af1-a347-7414eb03fced model=qwen2:0.5b address=http://localhost:11434
[2026-02-08 20:38:12] INFO llm_inference.py:459: request_id=b9daab51-ed89-4043-adcd-418edec8f1ab path=/api/v1/models/start status=200
INFO:     127.0.0.1:52940 - "POST /api/v1/models/start HTTP/1.1" 200 OK
INFO 02-08 20:39:42 [utils.py:261] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'model': 'Qwen/Qwen2-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 02-08 20:39:45 [model.py:541] Resolved architecture: Qwen2ForCausalLM
WARNING 02-08 20:39:45 [model.py:1833] Your device 'Tesla V100-PCIE-32GB' (with compute capability 7.0) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 02-08 20:39:45 [model.py:1885] Casting torch.bfloat16 to torch.float16.
INFO 02-08 20:39:45 [model.py:1561] Using max model len 32768
INFO 02-08 20:39:45 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 02-08 20:39:45 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:48 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:50 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.20.2:47479 backend=nccl
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:50 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:51 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen2-0.5B-Instruct...
[0;36m(EngineCore_DP0 pid=2841396)[0;0m ERROR 02-08 20:39:51 [fa_utils.py:86] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:52 [cuda.py:364] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:54 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:55 [default_loader.py:291] Loading weights took 0.63 seconds
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:39:55 [gpu_model_runner.py:4118] Model loading took 0.93 GiB memory and 3.293997 seconds
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:01 [backends.py:805] Using cache directory: /home/jiafq/.cache/vllm/torch_compile_cache/ed6d90913b/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:01 [backends.py:865] Dynamo bytecode transform time: 5.81 s
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:05 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.046 s
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:05 [monitor.py:34] torch.compile takes 6.86 s in total
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:06 [gpu_worker.py:356] Available KV cache memory: 11.18 GiB
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:06 [kv_cache_utils.py:1307] GPU KV cache size: 976,624 tokens
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:06 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 29.80x
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|‚ñç         | 2/51 [00:00<00:04, 11.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|‚ñä         | 4/51 [00:00<00:03, 12.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|‚ñà‚ñè        | 6/51 [00:00<00:03, 12.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|‚ñà‚ñå        | 8/51 [00:00<00:03, 12.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|‚ñà‚ñâ        | 10/51 [00:00<00:03, 12.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|‚ñà‚ñà‚ñå       | 13/51 [00:00<00:02, 16.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|‚ñà‚ñà‚ñà‚ñè      | 16/51 [00:01<00:02, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|‚ñà‚ñà‚ñà‚ñå      | 18/51 [00:01<00:02, 15.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|‚ñà‚ñà‚ñà‚ñâ      | 20/51 [00:01<00:02, 15.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22/51 [00:01<00:01, 14.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24/51 [00:01<00:01, 13.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27/51 [00:01<00:01, 16.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 30/51 [00:01<00:01, 18.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 32/51 [00:02<00:01, 18.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34/51 [00:02<00:01, 16.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 36/51 [00:02<00:00, 15.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38/51 [00:02<00:00, 14.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 40/51 [00:02<00:00, 14.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 43/51 [00:02<00:00, 16.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 46/51 [00:02<00:00, 18.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 48/51 [00:03<00:00, 17.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 50/51 [00:03<00:00, 16.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:03<00:00, 15.38it/s]
[0;36m(EngineCore_DP0 pid=2841396)[0;0m 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|‚ñé         | 1/35 [00:00<00:06,  5.38it/s]
Capturing CUDA graphs (decode, FULL):   9%|‚ñä         | 3/35 [00:00<00:03, 10.50it/s]
Capturing CUDA graphs (decode, FULL):  17%|‚ñà‚ñã        | 6/35 [00:00<00:01, 16.35it/s]
Capturing CUDA graphs (decode, FULL):  26%|‚ñà‚ñà‚ñå       | 9/35 [00:00<00:01, 19.24it/s]
Capturing CUDA graphs (decode, FULL):  34%|‚ñà‚ñà‚ñà‚ñç      | 12/35 [00:00<00:01, 21.13it/s]
Capturing CUDA graphs (decode, FULL):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 15/35 [00:00<00:00, 22.59it/s]
Capturing CUDA graphs (decode, FULL):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 18/35 [00:00<00:00, 22.72it/s]
Capturing CUDA graphs (decode, FULL):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/35 [00:01<00:00, 22.84it/s]
Capturing CUDA graphs (decode, FULL):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 24/35 [00:01<00:00, 21.97it/s]
Capturing CUDA graphs (decode, FULL):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 27/35 [00:01<00:00, 15.30it/s]
Capturing CUDA graphs (decode, FULL):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 30/35 [00:01<00:00, 16.88it/s]
Capturing CUDA graphs (decode, FULL):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 33/35 [00:01<00:00, 18.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:01<00:00, 17.91it/s]
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:12 [gpu_model_runner.py:5051] Graph capturing finished in 6 secs, took 6.49 GiB
[0;36m(EngineCore_DP0 pid=2841396)[0;0m INFO 02-08 20:40:12 [core.py:272] init engine (profile, create kv cache, warmup model) took 16.75 seconds
INFO 02-08 20:40:14 [llm.py:343] Supported tasks: ['generate']
[2026-02-08 20:40:14] INFO inference_core.py:498: LLMInferencer ÂàùÂßãÂåñÊàêÂäü: engine=vllm, model=qwen2-0.5b

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 102.83it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.94it/s, est. speed input: 20.97 toks/s, output: 104.76 toks/s]
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.86it/s, est. speed input: 20.97 toks/s, output: 104.76 toks/s]
[2026-02-08 20:40:14] INFO llm_inference.py:253: Â∑≤ÂêØÂä®Ê®°Âûã uid=cc34b04f-d7d5-4135-8cb9-387c23091a4b model_id=qwen2-0.5b engine=vllm address=http://localhost:8000
[2026-02-08 20:40:14] INFO llm_inference.py:459: request_id=53921bc9-3584-4365-88bf-df4aa083ffff path=/api/v1/models/start status=200
INFO:     127.0.0.1:59946 - "POST /api/v1/models/start HTTP/1.1" 200 OK
[2026-02-08 20:42:46] INFO llm_inference.py:459: request_id=3ab09c52-db8b-4f92-a593-e5815de2fc17 path=/ status=200
INFO:     127.0.0.1:47876 - "GET / HTTP/1.1" 200 OK
[2026-02-08 20:42:46] INFO llm_inference.py:459: request_id=74f5ada5-0742-4f22-b195-74bb0497ebfa path=/js/app.js status=304
INFO:     127.0.0.1:47876 - "GET /js/app.js HTTP/1.1" 304 Not Modified
[2026-02-08 20:42:46] INFO llm_inference.py:459: request_id=0f41a74e-883b-4a3e-83ee-ddd33c54285e path=/api/v1/models status=200
INFO:     127.0.0.1:47876 - "GET /api/v1/models HTTP/1.1" 200 OK
[2026-02-08 20:42:46] INFO llm_inference.py:459: request_id=7db31f4c-97d5-42c0-8b1c-6d07575be3d3 path=/api/v1/models/running status=200
INFO:     127.0.0.1:47890 - "GET /api/v1/models/running HTTP/1.1" 200 OK
[2026-02-08 20:42:58] INFO llm_inference.py:630: request_id=9133c43c-57ad-4c62-bc41-dba69235922a engine=vllm model=qwen2-0.5b prompt_len=4

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 658.76it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.10s/it, est. speed input: 0.22 toks/s, output: 112.55 toks/s]
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.10s/it, est. speed input: 0.22 toks/s, output: 112.55 toks/s]
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.10s/it, est. speed input: 0.22 toks/s, output: 112.55 toks/s]
[2026-02-08 20:43:08] INFO llm_inference.py:459: request_id=9133c43c-57ad-4c62-bc41-dba69235922a path=/api/v1/llm/generate status=200
INFO:     127.0.0.1:58496 - "POST /api/v1/llm/generate HTTP/1.1" 200 OK
INFO 02-08 20:46:32 [utils.py:261] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'model': 'Qwen/Qwen2-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 02-08 20:46:34 [model.py:541] Resolved architecture: Qwen2ForCausalLM
WARNING 02-08 20:46:34 [model.py:1833] Your device 'Tesla V100-PCIE-32GB' (with compute capability 7.0) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 02-08 20:46:34 [model.py:1885] Casting torch.bfloat16 to torch.float16.
INFO 02-08 20:46:34 [model.py:1561] Using max model len 32768
INFO 02-08 20:46:34 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=2842319)[0;0m INFO 02-08 20:46:34 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen2-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2842319)[0;0m INFO 02-08 20:46:37 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.20.2:37443 backend=nccl
[0;36m(EngineCore_DP0 pid=2842319)[0;0m INFO 02-08 20:46:37 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946]     raise ValueError(
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ERROR 02-08 20:46:37 [core.py:946] ValueError: Free memory on device cuda:0 (5.12/31.74 GiB) on startup is less than desired GPU memory utilization (0.5, 15.87 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[0;36m(EngineCore_DP0 pid=2842319)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=2842319)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 235, in init_device
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     self.requested_memory = request_memory(init_snapshot, self.cache_config)
[0;36m(EngineCore_DP0 pid=2842319)[0;0m                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2842319)[0;0m   File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 260, in request_memory
[0;36m(EngineCore_DP0 pid=2842319)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=2842319)[0;0m ValueError: Free memory on device cuda:0 (5.12/31.74 GiB) on startup is less than desired GPU memory utilization (0.5, 15.87 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[2026-02-08 20:46:37] ERROR llm_inference.py:511: ÂêØÂä®Ê®°ÂûãÂºÇÂ∏∏: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Traceback (most recent call last):
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 490, in api_start_model
    run_id, address = _start_model_impl(
                      ^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 224, in _start_model_impl
    inferencer = LLMInferencer(
                 ^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 497, in __init__
    self._adapter.check_service(self.model_name)
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 379, in check_service
    self._get_llm(model_name)
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 364, in _get_llm
    self._llm = LLM(**kwargs)
                ^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 334, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 172, in from_engine_args
    return cls(
           ^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 106, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 94, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 647, in __init__
    super().__init__(
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
    wait_for_engine_startup(
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-02-08 20:46:37] ERROR llm_inference.py:445: Êú™ÊçïËé∑ÂºÇÂ∏∏: Engine core initialization failed. See root cause above. Failed core proc(s): {}
NoneType: None
INFO:     127.0.0.1:51644 - "POST /api/v1/models/start HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
                                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 458, in add_request_id_and_log
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 490, in api_start_model
    run_id, address = _start_model_impl(
                      ^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 224, in _start_model_impl
    inferencer = LLMInferencer(
                 ^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 497, in __init__
    self._adapter.check_service(self.model_name)
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 379, in check_service
    self._get_llm(model_name)
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 364, in _get_llm
    self._llm = LLM(**kwargs)
                ^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 334, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 172, in from_engine_args
    return cls(
           ^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 106, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 94, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 647, in __init__
    super().__init__(
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
    wait_for_engine_startup(
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-02-08 20:49:32] INFO llm_inference.py:459: request_id=4696337f-ec1c-41c9-8d88-13e6bd3c59eb path=/api/v1/models/running/cc34b04f-d7d5-4135-8cb9-387c23091a4b/stop status=200
INFO:     127.0.0.1:43086 - "POST /api/v1/models/running/cc34b04f-d7d5-4135-8cb9-387c23091a4b/stop HTTP/1.1" 200 OK
[2026-02-08 20:49:33] INFO llm_inference.py:459: request_id=1a06f4fc-e488-4747-99bf-64980081443a path=/api/v1/models/running/97d60f69-cd63-4af1-a347-7414eb03fced/stop status=200
INFO:     127.0.0.1:43086 - "POST /api/v1/models/running/97d60f69-cd63-4af1-a347-7414eb03fced/stop HTTP/1.1" 200 OK
[2026-02-08 20:49:48] ERROR llm_inference.py:511: ÂêØÂä®Ê®°ÂûãÂºÇÂ∏∏: 403 Client Error. (Request ID: Root=1-6988866c-34149cdf166a2d8a75eaec63;52bcbbc2-ca4d-428c-8a5a-3231e8fd31f7)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes.
Your request to access model meta-llama/Llama-3.2-1B-Instruct has been rejected by the repo's authors.
Traceback (most recent call last):
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 403, in hf_raise_for_status
    response.raise_for_status()
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 490, in api_start_model
    for m in BUILTIN_MODELS
                      ^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 216, in _start_model_impl
    resolved = _ensure_model_downloaded(model_id)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 167, in _ensure_model_downloaded
    snapshot_download(repo_id=hf_repo, cache_dir=cache_dir)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py", line 330, in snapshot_download
    _inner_hf_hub_download(file)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py", line 306, in _inner_hf_hub_download
    return hf_hub_download(
           ^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 420, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-6988866c-34149cdf166a2d8a75eaec63;52bcbbc2-ca4d-428c-8a5a-3231e8fd31f7)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes.
Your request to access model meta-llama/Llama-3.2-1B-Instruct has been rejected by the repo's authors.
[2026-02-08 20:49:48] ERROR llm_inference.py:445: Êú™ÊçïËé∑ÂºÇÂ∏∏: 403 Client Error. (Request ID: Root=1-6988866c-34149cdf166a2d8a75eaec63;52bcbbc2-ca4d-428c-8a5a-3231e8fd31f7)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes.
Your request to access model meta-llama/Llama-3.2-1B-Instruct has been rejected by the repo's authors.
NoneType: None
INFO:     127.0.0.1:56398 - "POST /api/v1/models/start HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 403, in hf_raise_for_status
    response.raise_for_status()
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
                                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 458, in add_request_id_and_log
    code=500,
           ^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 490, in api_start_model
    for m in BUILTIN_MODELS
                      ^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/llm_inference.py", line 216, in _start_model_impl
    resolved = _ensure_model_downloaded(model_id)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pub/data/jiafq/rzm/project/wlw_llm_inference/inference_core.py", line 167, in _ensure_model_downloaded
    snapshot_download(repo_id=hf_repo, cache_dir=cache_dir)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py", line 330, in snapshot_download
    _inner_hf_hub_download(file)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py", line 306, in _inner_hf_hub_download
    return hf_hub_download(
           ^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/jiafq/tiger2/anaconda3/envs/rzm/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 420, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-6988866c-34149cdf166a2d8a75eaec63;52bcbbc2-ca4d-428c-8a5a-3231e8fd31f7)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Llama-3.2-1B-Instruct/resolve/9213176726f574b556790deb65791e0c5aa438b6/.gitattributes.
Your request to access model meta-llama/Llama-3.2-1B-Instruct has been rejected by the repo's authors.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2840659]
